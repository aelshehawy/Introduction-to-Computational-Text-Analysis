{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextasData_Session2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1znelpNf-RETmO2OPoLWYwuxr3fDNja56",
      "authorship_tag": "ABX9TyOhTGx+wftA4Ur0yHeJIewd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aelshehawy/text-as-data-computational-text-analysis-oxford/blob/main/Code/TextasData_Session2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6URx_rbh3ehz"
      },
      "source": [
        "# **Session 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIEQUo9J_4Zt"
      },
      "source": [
        "**Plan of the Coding Session**\n",
        "\n",
        "- Recap Basic Preprocessing Data (lowercase, remove punctuation, remove numbers)\n",
        "- Stopwords\n",
        "- Tokenization\n",
        "- Lemmatization\n",
        "- Stemming\n",
        "- Pos-Tagging\n",
        "- Word-Sense-Disambiguation and Named Entity Recognition\n",
        "- Opening up several files\n",
        "- Dictionaries\n",
        "- Pandas\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mj5GiR-cuPJk"
      },
      "source": [
        "![coding](https://media.giphy.com/media/FPbnShq1h1IS5FQyPD/giphy.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6ChMHSOwWoM"
      },
      "source": [
        "# NLP Pipleline until now \n",
        "## we have been focusing on preliminary cleaning methods\n",
        "\n",
        "1. lowercase\n",
        "2. remove puntcuation\n",
        "3. remove numbers\n",
        "\n",
        "## today we learn also about stopwords\n",
        "- **stopwords**\n",
        "- **tokenization**\n",
        "- **lemmatization**\n",
        "- **POS-Tagging**\n",
        "- **NER**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDKhbnXVwWoN"
      },
      "source": [
        "file1 = open('/content/drive/MyDrive/Colab Notebooks/Oxford Text Analysis/Data/ihaveadream.txt', 'r')\n",
        "file=file1.read() #we read the file in\n",
        "file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oAMYjbswWoO"
      },
      "source": [
        "#lets remove the stops \n",
        "file2=file.replace('\\n\\n','') #we check the structure of the text\n",
        "file2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjApAfeOwWoO"
      },
      "source": [
        "#lower case\n",
        "file3=file2.lower()\n",
        "file3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwzBx5sYwWoP"
      },
      "source": [
        "#remove punctuation\n",
        "import re\n",
        "file4 = re.sub('[^A-Za-z0-9]+', ' ', file3)\n",
        "file4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2V8QMmbRwWoP"
      },
      "source": [
        "#remove numbers\n",
        "file5 = re.sub(r'[0-9]+', \" \", file4)\n",
        "file5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fv-my1ZowWoP"
      },
      "source": [
        "# Stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dopQuChuwWoP"
      },
      "source": [
        "### What is NLTK? Why is it useful?\n",
        "\n",
        "\n",
        "Natural Language Toolkit. <br>\n",
        "Suit of libraries that help with tasks of Text Processing.<br>\n",
        "Can do different things, tokenize, lemmatize etc. more about that next week<br>\n",
        "\n",
        "\n",
        "*Stop Words:* A stop word is a commonly used word (such as “the”, “a”, “an”, “in”). \n",
        "Stopwords are words that do not add much meaning to a sentence. Commonly search engines has been programmed to ignore.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEQHCpv6wWoQ"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords') #you can also download all libraries in nltk at once\n",
        "from nltk.corpus import stopwords\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMCB-FR_wWoQ"
      },
      "source": [
        "english=stopwords.words('english')\n",
        "print(english)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v22EWsQcwWoQ"
      },
      "source": [
        "spanish=stopwords.words('spanish')\n",
        "print(spanish)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsfIhGP1wWoR"
      },
      "source": [
        "german=stopwords.words('german')\n",
        "print(german)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHI0kr4pwWoR"
      },
      "source": [
        "arabic=stopwords.words('arabic')\n",
        "print(arabic)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e8IPFb8wWoR"
      },
      "source": [
        "stops= stopwords.words('english')\n",
        "\n",
        "file5_words = file5.split() #cleaned martin luther king speech from above\n",
        "print(file5_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgGmWs0vwWoR"
      },
      "source": [
        "file5_words=[word for word in file5_words if word not in stops] # see difference between this step and the step above\n",
        "file5_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ls4J5uphwWoS"
      },
      "source": [
        "## Do everything in one step using several text files in a directory:\n",
        "\n",
        "### We need functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssPhCnsjH_-B"
      },
      "source": [
        "def clean(text):\n",
        "'how would you fill in this function' #excercise after class!\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "508Mkyv1IAQ2"
      },
      "source": [
        "import codecs\n",
        "import os\n",
        "\n",
        "articles_new=[]\n",
        "for filename in os.listdir(\"/content/drive/MyDrive/Colab Notebooks/Oxford Text Analysis/Data/Data Day 1\"):\n",
        "   # print(filename)\n",
        "    if \".txt\" in filename:\n",
        "        document = open(\"/content/drive/MyDrive/Colab Notebooks/Oxford Text Analysis/Data/Data Day 1/\"+filename,\"r\")\n",
        "       # print(document)\n",
        "        document2 = document.read()\n",
        "       # print(document2)\n",
        "        #how do I use the pipeline to clean? this is todays excercise!\n",
        "        articles_new.append(document2)\n",
        "\n",
        "print(articles_new) \n",
        "print(len(articles_new))      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6UY7LsgALJ4"
      },
      "source": [
        "# Tokenization Session - using NLTK package"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfIPlwI7ALJ5"
      },
      "source": [
        "Tokenization is splitting a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words or terms. Each of these smaller units are called tokens.\n",
        "\n",
        "- Tokanization has advantages over Python .split() function especially when it comes to sentence tokenization\n",
        "- The tokenizer is trained to discover end of sentence and tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "BPAeTOdKALJ6"
      },
      "source": [
        "article = 'In computer science, lexical analysis, lexing or tokenization is the process of \\\n",
        "converting a sequence of characters (such as in a computer program or web page) into a \\\n",
        "sequence of tokens (strings with an assigned and thus identified meaning). A program that \\\n",
        "performs lexical analysis may be termed a lexer, tokenizer,[1] or scanner, though scanner \\\n",
        "is also a term for the first stage of a lexer. A lexer is generally combined with a parser, \\\n",
        "which together analyze the syntax of programming languages, web pages, and so forth.'\n",
        "\n",
        "article2 = 'ConcateStringAnd123 ConcateSepcialCharacter_!@# !@#$%^&*()_+ 0123456'\n",
        "\n",
        "article3 = 'It is a great moment from 10 a.m. to 1 p.m. every weekend.'\n",
        "\n",
        "\n",
        "# Example inspired by Edward MA blogging on Medium"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIStuXHtALJ8"
      },
      "source": [
        "import re\n",
        "\n",
        "for doc in [article, article2, article3]: # for every document \n",
        "    print('Original Article:', (doc)) #print the original article\n",
        "    print()#print empty space\n",
        "\n",
        "    sentences = re.split('(\\.|!|\\?)', doc) #split by these items using regex\n",
        "    \n",
        "    for i, s in enumerate(sentences): #count number of sentences\n",
        "        print('-->Sentence %d: %s' % (i, s)) #place holders %d\n",
        "\n",
        "        #different ways "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXqKmugfMANL"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "print('NTLK Version: %s' % nltk.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuOcSUjQALJ9"
      },
      "source": [
        "for article in [article, article2, article3]:\n",
        "    print('Original Article: %s' % (article))\n",
        "    print()\n",
        "\n",
        "    doc = sent_tokenize(article)\n",
        "    for i, token in enumerate(doc):\n",
        "        print('-->Sentence %d: %s' % (i, token))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LH4EheKRALJ9"
      },
      "source": [
        "text = \"\"\"Amazing day... however, I still need a good night sleep. I will see you tomorrow for sure. Bye.Bye\"\"\"\n",
        "# Splits at '.' \n",
        "text.split('. ') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrFb8Ny7ALJ-"
      },
      "source": [
        "tokenized_text = nltk.sent_tokenize(text)\n",
        "tokenized_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx_NKKZJALJ-"
      },
      "source": [
        "### Lets work with a new dataset and discover it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "iWlUcNjAALJ_"
      },
      "source": [
        "import codecs\n",
        "dataset = codecs.open(\"/content/drive/MyDrive/Colab Notebooks/Oxford Text Analysis/Data/rt_dataset_small\", \"r\", \"utf-8\").read().strip().split(\"\\n\") \n",
        "# we split by line breaks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2QzjFvFALJ_"
      },
      "source": [
        "print(dataset[0])\n",
        "print(dataset[1])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ek06ZmMALKA"
      },
      "source": [
        "print (len(dataset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tI20ZCHALKB"
      },
      "source": [
        "# let's start with some real NLP\n",
        "\n",
        "# let's focus on a specific article, for example\n",
        "\n",
        "article = dataset[50].split(\"\\t\")[3]\n",
        "print (article)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U62QNGY5ALKC"
      },
      "source": [
        "print (type(article))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOcBEZKyALKC"
      },
      "source": [
        "sentences = nltk.sent_tokenize(article) # <-- documentation for this command: http://www.nltk.org/_modules/nltk/tokenize.html\n",
        "\n",
        "# for checking what you're getting back from a library, run these commands\n",
        "print (type(sentences))\n",
        "print (len(sentences))\n",
        "print(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4HizCGDALKC"
      },
      "source": [
        "# let us consider a single sentence - how do we do that? ## use the 5th sentence\n",
        "\n",
        "sentence = sentences[4]\n",
        "print(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KPY0l-sALKD"
      },
      "source": [
        "# let's divide the sentence in tokens (aka single words)\n",
        "tokenized_sentence = nltk.word_tokenize(sentence)\n",
        "\n",
        "print (tokenized_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVGGaVsiALKD"
      },
      "source": [
        "# lower-casing the sentence\n",
        "without_capital_letters = [word.lower() for word in tokenized_sentence]\n",
        "\n",
        "print (without_capital_letters)\n",
        "\n",
        "# homework: write a for-loop for doing the same thing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eDAdV9kALKE"
      },
      "source": [
        "#remove stopwords\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "without_stop_words = [word for word in without_capital_letters if word not in stop]\n",
        "\n",
        "print (without_stop_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mDKbxM4ALKE"
      },
      "source": [
        "import string\n",
        "exclude = set(string.punctuation) # if you see this not part of the punctuation -->”\n",
        "\n",
        "\n",
        "without_punct = [word for word in without_stop_words if word not in exclude]\n",
        "\n",
        "print (without_punct)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9huG_OrZALKE"
      },
      "source": [
        "## Let's take a look at our cleaning pipeline\n",
        "\n",
        "missing remove stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WhfOv0RALKF"
      },
      "source": [
        "dataset = codecs.open(\"/content/drive/MyDrive/Colab Notebooks/Oxford Text Analysis/Data/rt_dataset_small\", \"r\", \"utf-8\").read().strip().split(\"\\n\") \n",
        "article=dataset[1]\n",
        "article"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NU7gPQ0UALKF"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/Oxford Text Analysis/Data/rt_dataset_small') as f:\n",
        "    counter = Counter(f.read().strip().split())\n",
        "#strip removes spaces at the beginning and end\n",
        "print(counter.most_common(20))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JulhBiGNWTwx"
      },
      "source": [
        "This is why we remove stopwords."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvQuOUmwyc-6"
      },
      "source": [
        "Naive Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "F9b6rfhqALKG"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "exclude = set(string.punctuation)\n",
        "exclude.add(\"‘\")\n",
        "exclude.add(\"“\") # we add to pre-defined punctuation\n",
        "\n",
        "import re\n",
        "\n",
        "def clean1(x): \n",
        "    x=x.replace('\\n\\n','') # remove the line breaks\n",
        "    x=x.lower()# lower text\n",
        "    x = ''.join(ch for ch in x if ch not in exclude) #remove punctuation\n",
        "    x=re.sub('[0-9]+', '', x) # remove numbers\n",
        "    x=x.split() #split words naively\n",
        "    x=[word for word in x if word not in stopwords.words('english')]#remove stopwords\n",
        "   #x=\" \".join(str(x) for x in x) # you can do this if you want to remove list structure\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zn-BTyrIALKH"
      },
      "source": [
        "cleaned=clean1(article)\n",
        "print(cleaned)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0nbizjAyfoE"
      },
      "source": [
        "Building An NLP Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "QlloImZnALKI"
      },
      "source": [
        "def nlp_pipeline1(text):\n",
        "    \n",
        "    text=text.lower()\n",
        "    \n",
        "    #tokenize words for each sentence\n",
        "    text = nltk.word_tokenize(text)\n",
        "    \n",
        "    # remove punctuation and numbers\n",
        "    #text = [token for token in text if token.isalpha()] #method returns True if all the characters are alphabet letters\n",
        "    \n",
        "    # remove stopwords - be careful with this step    \n",
        "    #text = [token for token in text if token not in stop_words]\n",
        "\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPhd48FbALKJ"
      },
      "source": [
        "cleaned1=nlp_pipeline1(article)\n",
        "print(cleaned1)\n",
        "# discrepency 'https', ':', '//t.co/whndqjykmppic.twitter.com/qztfwcsix4' and httpstcowhndqjykmppictwittercomqztfwcsix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xj4jFwHVALKM"
      },
      "source": [
        "**We keep adding to this pipeline once we learn new preprocessing methods**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "-pLH5trKALKN"
      },
      "source": [
        "def nlp_pipeline2(text):\n",
        "    \n",
        "    text=text.lower()\n",
        "    \n",
        "    #tokenize words for each sentence\n",
        "    text = nltk.word_tokenize(text)\n",
        "    \n",
        "    # remove punctuation and numbers\n",
        "    text = [token for token in text if token.isalpha()]#The isalpha() keeps here caracters in the string are alphabets\n",
        "    \n",
        "    # remove stopwords - be careful with this step    \n",
        "    text = [token for token in text if token not in stop_words]\n",
        "\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRha3od8ALKN"
      },
      "source": [
        "cleaned2=nlp_pipeline2(article)\n",
        "print(cleaned2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0CD7DTwALKN"
      },
      "source": [
        "#discrepencies between both lists - can be homework or exam\n",
        "def diff(first, second):\n",
        "        second = set(second)\n",
        "        return [item for item in first if item not in second]\n",
        "\n",
        "diff(cleaned1,cleaned2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpMPWzmqALKO"
      },
      "source": [
        "- take care of weird punctuation in the beginning\n",
        "- we are going to see in sentiment analysis why dont etc. can be of important"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhJJSUJ6AgMk"
      },
      "source": [
        "# Lemmatization and Stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zarUDtxAgM1"
      },
      "source": [
        "#some needed libraries\n",
        "import numpy as np\n",
        "from numpy import *\n",
        "from numpy import random # random data\n",
        "import csv\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from itertools import chain\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import codecs\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import pprint\n",
        "import random\n",
        "from urllib import request\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import brown\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "from nltk.stem import SnowballStemmer\n",
        "snowball_stemmer = SnowballStemmer(\"english\")\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGDRBFuSAgM5"
      },
      "source": [
        "## Lets take a look at what we have learned until now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "d--ie-zeAgM6"
      },
      "source": [
        "dataset = codecs.open(\"/content/drive/MyDrive/Colab Notebooks/Oxford Text Analysis/Data/rt_dataset_small\", \"r\", \"utf-8\").read().strip().split(\"\\n\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbIFI4nfzYDL"
      },
      "source": [
        "Overview of what each pre-processing step does until now"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5Sg9n2iAgM6"
      },
      "source": [
        "# take a single article\n",
        "article = dataset[50].split(\"\\t\")[3]\n",
        "\n",
        "\n",
        "# split into sentences\n",
        "sentences = nltk.sent_tokenize(article) \n",
        "\n",
        "# take one single sentence (I'm adding a number at the end - just for testing that stripping numbers work fine)\n",
        "sentence = sentences[4] + \" 5\"\n",
        "print(\" \")\n",
        "\n",
        "print (sentence)\n",
        "\n",
        "# word tokenizer\n",
        "sentence = nltk.word_tokenize(sentence)\n",
        "print(\" \")\n",
        "\n",
        "print (\"Tokenizer:\\n\",sentence)\n",
        "\n",
        "\n",
        "# lowering words\n",
        "sentence = [word.lower() for word in sentence]\n",
        "print(\" \")\n",
        "\n",
        "print (\"lower-cased:\\n\",sentence)\n",
        "\n",
        "\n",
        "# defining stop-words (it's a list)\n",
        "stop_word_list = stopwords.words('english')\n",
        "\n",
        "\n",
        "# removing stopwords\n",
        "sentence = [word for word in sentence if word not in stop_word_list]\n",
        "print(\" \")\n",
        "\n",
        "print (\"without stopwords:\\n\",sentence)\n",
        "\n",
        "\n",
        "# defining punctuation to be removed\n",
        "exclude = set(string.punctuation)\n",
        "\n",
        "#\n",
        "sentence = [token for token in sentence if token not in exclude]\n",
        "print(\" \")\n",
        "\n",
        "print (\"without punctuation:\\n\",sentence)\n",
        "\n",
        "\n",
        "#remove punctuations and numbers\n",
        "sentence = [word for word in sentence if word.isalpha()]\n",
        "print(\" \")\n",
        "print(\"without punctuation and numbers:\\n\",sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkT91Pw0AgM7"
      },
      "source": [
        "# Stemming\n",
        "\n",
        "## We reduce the word to its stem basically, we reduce words to their core root\n",
        "\n",
        "- Put in simple terms: several conditions that the stemmer evaluates\n",
        "- Mapping a group of words to the same stem\n",
        "- May result in non-words, rather stems of the actual words\n",
        "- chopping off the last part of the word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKDLE2pRAgM9"
      },
      "source": [
        "## When can it be helpful\n",
        "\n",
        "- Information retrieval setting and you want to boost your algorithm’s recall. (Recall is defined as the number of relevant documents retrieved by a search divided by the total number of existing relevant documents)\n",
        "- Query Expansion: Search Environments which refers to that when a user inputs a query. It is used to expand or enhance the query to match additional documents\n",
        "- World usage analysis in a corpus, we wish to condense words to reduce variability.\n",
        "- Sentiment Analysis\n",
        "- Classification tasks\n",
        "\n",
        "## Take care of overstemming and understemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pN2vWwNAgM-"
      },
      "source": [
        "list1=[\"university\", \"universal\", \"universities\", \"universe\"]\n",
        "stem_list1 = [snowball_stemmer.stem(word) for word in list1]\n",
        "stem_list1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrLQttDOAgM_"
      },
      "source": [
        "list2=[ \"data\" , \"datum\",\"date\"]\n",
        "stem_list2 = [snowball_stemmer.stem(word) for word in list2]\n",
        "stem_list2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fv_XXNB8AgNA"
      },
      "source": [
        "Difference between the snowball, porter, and lancaster stemming algorithms:\n",
        "- Three main stemming mechanisms iare **Porter, Snowball(Porter2), Lancaster**\n",
        "- Lancaster is more aggressive that porter\n",
        "- Snowball stemmer has applications in different European languages\n",
        "- Russian is surprisingly easy to process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acfn_KtvAgNA"
      },
      "source": [
        "# import the library\n",
        "from nltk.stem import SnowballStemmer\n",
        "snowball_stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "print (\"tokenized:\\n\",sentence)\n",
        "\n",
        "stem_sentence = [snowball_stemmer.stem(word) for word in sentence]\n",
        "\n",
        "print (\"stemmed:\\n\",stem_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "mA4eOQeFAgNB"
      },
      "source": [
        "\n",
        "import nltk\n",
        "import re\n",
        "import pprint\n",
        "import random\n",
        "from urllib import request\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import brown\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "article = \"\"\"Donald John Trump (born June 14, 1946) is the 45th and current president of the United States. Before entering politics, he was a businessman and television personality.\n",
        "Trump was born and raised in Queens, a borough of New York City, and received a bachelor's degree in economics from the Wharton School. He took charge of his family's real-estate business in 1971, renamed it The Trump Organization, and expanded its operations from Queens and Brooklyn into Manhattan. The company built or renovated skyscrapers, hotels, casinos, and golf courses. Trump later started various side ventures, mostly by licensing his name. He produced and hosted The Apprentice, a reality television series, from 2003 to 2015. As of 2020, Forbes estimated his net worth to be $2.1 billion.[a]\n",
        "Trump entered the 2016 presidential race as a Republican and defeated 16 other candidates in the primaries. His political positions have been described as populist, protectionist, and nationalist. Despite not being favored in most forecasts, he was elected over Democratic nominee Hillary Clinton, although he lost the popular vote. \"\"\"\n",
        "# from Wikipedia https://en.wikipedia.org/wiki/Donald_Trump\n",
        "\n",
        "tokens = word_tokenize(article)\n",
        "\n",
        "snowball = SnowballStemmer(\"english\")\n",
        "porter = nltk.PorterStemmer()\n",
        "lancaster = nltk.LancasterStemmer()\n",
        "\n",
        "snowball_output = [snowball_stemmer.stem(t) for t in tokens]             \n",
        "porter_output = [porter.stem(t) for t in tokens]             \n",
        "lancaster_output = [lancaster.stem(t) for t in tokens]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eeMkg2lAgNC"
      },
      "source": [
        "print(porter_output)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNjaLOi6AgND"
      },
      "source": [
        "\n",
        "print(snowball_output)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmJK60TSAgNF"
      },
      "source": [
        "print(lancaster_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnbtG4lKAgNF"
      },
      "source": [
        "#A list of words to be stemmed\n",
        "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\",\"stabil\",\"destabilize\",\"misunderstanding\",\"railroad\",\"moonlight\",\"football\"]\n",
        "print(\"{0:20}{1:20}{2:20}\".format(\"Word\",\"Snowball Stemmer\",\"lancaster Stemmer\"))\n",
        "for word in word_list:\n",
        "    print(\"{0:20}{1:20}{2:20}\".format(word,snowball_stemmer.stem(word),lancaster.stem(word)))\n",
        "    \n",
        "#https://www.datacamp.com/community/tutorials/stemming-lemmatization-python"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJo2UTvkAgNG"
      },
      "source": [
        "# Lemmatization\n",
        "## Setting words to their dictionary form\n",
        "The process of grouping together the inflected forms of a word as a single\n",
        "item.\n",
        "\n",
        "\n",
        "## Employing Wordnet - large lexical database of English\n",
        "\n",
        "read more about wordnet here: https://wordnet.princeton.edu/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoJ5tsveAgNH"
      },
      "source": [
        "## When is it benefitial to lemmatize?\n",
        "\n",
        "- Topic modeling: relies on the distribution of words, the identification of which is dependent on a string match between words, which is achieved by lemmatizing their forms so that all variants are consistent across documents\n",
        "- Sentiment Analysis\n",
        "- Classification tasks\n",
        "\n",
        "- The general rule: if it does not help your performance, do not lemmatize.\n",
        "\n",
        "- Some sentiment analysis methods can have different performance according to lemmitazation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "5tvxg7kkAgNH"
      },
      "source": [
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "wordnet_lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kc7iDLsHJ4p"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ye5M34qAgNI"
      },
      "source": [
        "list3=[\"fishes\", \"fishing\", \"fish\"]\n",
        "lemma_list3 = [wordnet_lemmatizer.lemmatize(word) for word in list3]\n",
        "lemma_list3\n",
        "\n",
        "stem_list3 = [snowball_stemmer.stem(word) for word in list3]\n",
        "stem_list3 # here we map a group of words to the same stem\n",
        "\n",
        "print(\"lemmatization:\", lemma_list3)\n",
        "print(\"stemming:\", stem_list3)# here we map a group of words to the same stem"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nchHdlwfAgNI"
      },
      "source": [
        "party=[\"party\", \"partying\",\"parties\"]\n",
        "lemma_party = [wordnet_lemmatizer.lemmatize(word) for word in party]\n",
        "stem_party = [snowball_stemmer.stem(word) for word in party]\n",
        "\n",
        "print(\"lemmatization:\", lemma_party)\n",
        "print(\"stemming:\", stem_party)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "QBBoIitVAgNJ"
      },
      "source": [
        "# see the difference it doesnt treat verbs for example similarly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyRHXBOmAgNJ"
      },
      "source": [
        "list4=[\"connections\",\"connecting\",\"connect\",\"connected\",\"connection\"]\n",
        "lemma_list4 = [wordnet_lemmatizer.lemmatize(word) for word in list4]\n",
        "stem_list4 = [snowball_stemmer.stem(word) for word in list4]\n",
        "print(\"lemmatization:\", lemma_list4)\n",
        "print(\"stemming:\", stem_list4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpfVkN-0AgNJ"
      },
      "source": [
        "\n",
        "lemma_sent = [wordnet_lemmatizer.lemmatize(word) for word in sentence]\n",
        "stem_sentence = [snowball_stemmer.stem(word) for word in sentence]\n",
        "\n",
        "print (\"stemmed:\\n\", stem_sentence)\n",
        "print(\" \")\n",
        "print (\"lemmatized:\\n\",lemma_sent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_iO-qQgAgNK"
      },
      "source": [
        "#A list of words to be stemmed and lemmatized\n",
        "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\",\"stabil\",\"destabilize\",\\\n",
        "             \"misunderstanding\",\"railroad\",\"moonlight\",\"football\",\"party\",\"extreme\",\\\n",
        "             \"muslim\",\"islam\",\"political\",\"politics\",\"perfect\"]\n",
        "print(\"{0:20}{1:20}{2:20}\".format(\"Word\",\"Snowball Stemmer\",\"Lemmatizing\"))\n",
        "for word in word_list:\n",
        "    print(\"{0:20}{1:20}{2:20}\".format(word,snowball_stemmer.stem(word),wordnet_lemmatizer.lemmatize(word)))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9lK1xrDAgNM"
      },
      "source": [
        "\n",
        "print('Original Article: %s' % (article))\n",
        "print()\n",
        "\n",
        "for token in tokens:\n",
        "    lemmatized_token = wordnet_lemmatizer.lemmatize(token)\n",
        "    \n",
        "    if token != lemmatized_token:\n",
        "        print('Original : %s, New: %s' % (token, lemmatized_token))\n",
        "        \n",
        "#inspired by edward ma"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9EwivoAAgNN"
      },
      "source": [
        "**Always check if employing those methods will improve your performance!**\n",
        "\n",
        "*Stemming can sometimes combine together words with substantively different meanings:<br>\n",
        "(“college students partying”, and “political parties”), which might be misleading in practice (Denny and Spirling 2018)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPmtRDN_AgNN"
      },
      "source": [
        " **Always Think about the order in your NLP Pipeline**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTi9yjxxD1__"
      },
      "source": [
        "# Pos-Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6De8WyjD2AA"
      },
      "source": [
        "### In POS tagging the probability of a POS in the previous words conditions the probability of the tag for the current word.\n",
        "\n",
        "check categories of pos tagger here: https://www.nltk.org/book/ch05.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "yLmAUftAD2AB"
      },
      "source": [
        "sentence=\"I need access to the cloud\"\n",
        "print(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kOpr9AQD2AB"
      },
      "source": [
        "- is \"access here a noun or a verb\"\n",
        "\n",
        "Two probablities evaluated:\n",
        "1. Probability of access being a verb or a noun\n",
        "2. Probablity of a verb following a noun"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azCs9NeuIoD1"
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('tagsets')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riRKPmImD2AC"
      },
      "source": [
        "sent = nltk.word_tokenize(sentence)\n",
        "sent = nltk.pos_tag(sent)\n",
        "print(sent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SD_6KhSxD2AD"
      },
      "source": [
        "nltk.help.upenn_tagset(\"PRP\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "in7BkCQuD2AE"
      },
      "source": [
        "import codecs, nltk, string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "dataset = codecs.open(\"/content/drive/MyDrive/Colab Notebooks/Oxford Text Analysis/Data/rt_dataset_small\", \"r\", \"utf-8\").read().strip().split(\"\\n\")\n",
        "\n",
        "article = dataset[50].split(\"\\t\")[3]\n",
        "\n",
        "# split into sentences\n",
        "sentences = nltk.sent_tokenize(article) \n",
        "\n",
        "sentence = nltk.word_tokenize(sentences[4])\n",
        "\n",
        "# you use the pos-tagger (it gives you back a list of tuples (word,pos))\n",
        "pos_sentence = nltk.pos_tag(sentence)\n",
        "print(\"pos-tagged\",pos_sentence)\n",
        "\n",
        "lemma_word = [wordnet_lemmatizer.lemmatize(token.lower(),\"x\") if \"x\" in pos else wordnet_lemmatizer.lemmatize(token.lower()) for token,pos in pos_sentence]\n",
        "print()\n",
        "print (\"lemmatized\", lemma_word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfYCnDoOD2AG"
      },
      "source": [
        "nltk.help.upenn_tagset(\"RB\") #if I want to know what a tag stands for"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynLqy-nfD2AG"
      },
      "source": [
        "## Expanding our Pipeline with things we have learned up until now"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "xdtFIUlfD2AG"
      },
      "source": [
        "\n",
        "\n",
        "exclude = set(string.punctuation)\n",
        "stop_word_list = stopwords.words('english')\n",
        "\n",
        "# input should be a string\n",
        "def nlp_pipeline1(text):\n",
        "    \n",
        "    # if you want you can split in sentences - i'm usually skipping this step\n",
        "    # text = nltk.sent_tokenize(text) \n",
        "    text=text.lower()\n",
        "    \n",
        "    #tokenize words for each sentence\n",
        "    text = nltk.word_tokenize(text)\n",
        "   \n",
        "    # remove punctuation and numbers\n",
        "    text = [token for token in text if token not in exclude and token.isalpha()]\n",
        "    \n",
        "    # remove stopwords - be careful with this step    \n",
        "    text = [token for token in text if token not in stop_word_list]\n",
        "    \n",
        "        # pos tagger\n",
        "    text = nltk.pos_tag(text)\n",
        "\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bjf4gUdEgfZ"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "un7MorjYD2AK"
      },
      "source": [
        "# Word-Sense-Disambiguation\n",
        "\n",
        "### WSD is the task of assigning a sense to a word, given a context.\n",
        "\n",
        "- Words are ambigous\n",
        "- Language is very contextual and the meanings of the words depend upon the context in which you are using it.\n",
        "- The sense of a word is a way of identifying how we use a given word by associating its definition\n",
        "\n",
        "read more: https://www.linkedin.com/pulse/wordnet-word-sense-disambiguation-wsd-nltk-aswathi-nambiar/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCUot9qqD2AL"
      },
      "source": [
        "### How does it work?\n",
        "\n",
        "- using Thesaurus (WordNet is a large lexical database of English.)\n",
        "- Wordnet includes for example synonyms and annonyms\n",
        "- using Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjaC4aGSD2AL"
      },
      "source": [
        "For example, consider the two sentences.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3QIXoZzD2AL"
      },
      "source": [
        "**lesk** is one of the algorithms that help disambiguate and estimate the sense of words in a sentence\n",
        "\n",
        "1. Retrieve all sense definitions of target word \n",
        "2. Compare each sense definition with the sense definitions of the other words in context \n",
        "3. Choose the sense with the highest overlap\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "TptxyGOtD2AM"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.wsd import lesk\n",
        "#lesk is one of the algorithms that help disambiguate and estimate the sense of words in a sentence\n",
        "\n",
        "sent1=\"the sea bass is delicious\"\n",
        "sent2=\"musical bass\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OktGDOqLD2AM"
      },
      "source": [
        "sent2.split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0XyFKuzD2AM"
      },
      "source": [
        "lesk(sent1.split(),\"bass\",\"n\")\n",
        "\n",
        "#lemma.pos.countnumber"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IP-nJ2oD2AN"
      },
      "source": [
        "print(lesk(sent2.split(),\"bass\",\"n\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "dv2Odv--D2AO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huG7pwT7D2AO"
      },
      "source": [
        "#rt dataset\n",
        "article = dataset[261].split(\"\\t\")[3]\n",
        "\n",
        "print (article)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "dTJuiDLLD2AO"
      },
      "source": [
        "def nlp_pipeline(text):\n",
        "    \n",
        "    # if you want you can split in sentences - i'm usually skipping this step\n",
        "    # text = nltk.sent_tokenize(text) \n",
        "    \n",
        "    #tokenize words for each sentence\n",
        "    text = nltk.word_tokenize(text)\n",
        "    \n",
        "    # pos tagger\n",
        "    text = nltk.pos_tag(text)\n",
        "\n",
        "    # lemmatizer\n",
        "    text = [wordnet_lemmatizer.lemmatize(token.lower(),\"v\")if \"V\" in pos else wordnet_lemmatizer.lemmatize(token.lower()) for token,pos in text]\n",
        "    \n",
        "    # remove punctuation and numbers\n",
        "    text = [token for token in text if token not in exclude and token.isalpha()]\n",
        "    \n",
        "    # remove stopwords - be careful with this step    \n",
        "    text = [token for token in text if token not in stop_word_list]\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "    \n",
        "# let's use our pipeline!\n",
        "clean_article = nlp_pipeline(article)\n",
        "print (clean_article)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-az19AVLD2AP"
      },
      "source": [
        "## Synset instances are the groupings of synonymous words that express the same concept."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUnYMsrmD2AP"
      },
      "source": [
        "\n",
        "# word sense disambiguation\n",
        "\n",
        "# check documentation: http://www.nltk.org/howto/wordnet.html\n",
        "\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "# let's isolate each word - you do this using a set (another type of object in python)\n",
        "\n",
        "unique_words = set(clean_article)\n",
        "\n",
        "print(unique_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7GuECzbXw0r"
      },
      "source": [
        "# let's check how many senses each word has\n",
        "for word in unique_words:\n",
        "    print (word, len(wn.synsets(word)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhtQvWAnD2AQ"
      },
      "source": [
        "word = \"bank\"\n",
        "\n",
        "senses = wn.synsets(word)\n",
        "\n",
        "for sense in senses:\n",
        "    # get definition of sense\n",
        "    print(word, sense.definition())\n",
        "    \n",
        "    # get a textual example\n",
        "    print(\"textual example\",sense.examples())\n",
        "    \n",
        "    # get hypernymy - belonging to a higher rank\n",
        "    print(\"hypernymy\", sense.hypernyms())\n",
        "\n",
        "    # get  - examples\n",
        "    print(\"hyponyms\",sense.hyponyms())\n",
        "        \n",
        "    # this is a way of getting synonyms - there are others\n",
        "    print (\"synonyms\",sense.lemma_names())\n",
        "    \n",
        "    # this is for getting antonyms - works especially with adjectives \n",
        "    print (\"antonyms\",sense.lemmas()[0].antonyms())#opposite meaning\n",
        "    print ()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "WKBq1lJzD2AR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvAkRcq0D2AS"
      },
      "source": [
        "word = \"bank\"\n",
        "\n",
        "senses = wn.synsets(word)\n",
        "senses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4Or-_3pD2AV"
      },
      "source": [
        "for sense in senses[1:]:\n",
        "    # get definition of sense\n",
        "    definition = sense.definition()    \n",
        "    print (definition)\n",
        "    # take all hypernyms, hyponyms and synonyms - you need to do a bit of cleaning    \n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqiFSfiTMv7V"
      },
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.wsd import lesk\n",
        "sent = ['I', 'went', 'to', 'the', 'bank', 'to', 'deposit', 'money', '.']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvRIx9o9N6F0"
      },
      "source": [
        "print(lesk(sent, 'bank'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9EanljJD2AX"
      },
      "source": [
        "# Named-Entity Recognition\n",
        "\n",
        "## NER is a subtask of information extraction\n",
        "\n",
        "seeks to locate and classify pieces of  text into predefined categories such as the names of:\n",
        "- persons \n",
        "- organizations \n",
        "- locations\n",
        "- expressions of times \n",
        "- quantities\n",
        "- monetary values\n",
        "- percentages\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LV2LejeTD2AY"
      },
      "source": [
        "## How does it work?\n",
        "\n",
        "We talked about regular expressions remember\n",
        "\n",
        "**regular expression to extract:**\n",
        "- telephone numbers\n",
        "- E-mails\n",
        "- Dates\n",
        "- Prices\n",
        "- Locations (e.g., word + “river” indicates a river -> Hudson river)\n",
        "\n",
        "**context patterns**\n",
        "\n",
        "- [Person] earns [Money]\n",
        "- [PERSON] joined [ORGANIZATION]\n",
        "\n",
        "**sequence in the sentence**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvWWaH6uD2AY"
      },
      "source": [
        "import codecs, nltk\n",
        "\n",
        "dataset = codecs.open(\"/content/drive/MyDrive/Colab Notebooks/Oxford Text Analysis/Data/rt_dataset_small\", \"r\", \"utf-8\").read().strip().split(\"\\n\")\n",
        "\n",
        "\n",
        "print(dataset)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXd0cklPZIA9"
      },
      "source": [
        "# how to quickly find an article from the dataset\n",
        "for k in range(len(dataset)):\n",
        "    article = dataset[k]\n",
        "    if \"Trump\" in article and \"Hillary\" in article:\n",
        "        print (article.split(\"\\t\")[3])\n",
        "        print (k)\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lG6puw7sOR_v"
      },
      "source": [
        "import nltk\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UxpFlm-D2AY"
      },
      "source": [
        "# named entity recognition\n",
        "\n",
        "article = dataset[4].split(\"\\t\")[3]\n",
        "\n",
        "# first step you tokenize (read documentation to know the input of NER)\n",
        "article = nltk.word_tokenize(article)\n",
        "\n",
        "# you use the pos-tagger (it gives you back a list of tuples (word,pos))\n",
        "pos_article = nltk.pos_tag(article)\n",
        "\n",
        "# then you use the NER library\n",
        "ner = nltk.ne_chunk(pos_article)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CPxXK9COf1M"
      },
      "source": [
        "print(pos_article)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-z83QN7sOhtz"
      },
      "source": [
        "print(ner)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "c7xd7pEQD2AZ"
      },
      "source": [
        "#just visualization\n",
        "ner = [x for x in ner if type(x) == nltk.tree.Tree]\n",
        "print (ner)\n",
        "#GPE geopolitical entity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oa36kl5bEoSl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbizWc2CD2AZ"
      },
      "source": [
        "# Entity Linking\n",
        "\n",
        "- Linked Data is a way of publishing data on the (Semantic) Web \n",
        "- Linked data source created by:  Extracting structured information from Wikipedia, Using “infobox” of the articles \n",
        "- Given a mention of an entity in a document, link it to an entry in a Knowledge Base.\n",
        "\n",
        "**Note:**\n",
        "\n",
        "- Not all entities are in knowledge bases\n",
        "- Mentions could be ambiguous\n",
        "\n",
        "Yesterday I watched the debate between Clinton and Sanders. --> which Clinton?\n",
        "\n",
        "**How can we get over this issue?**\n",
        "\n",
        "- Popularity - give me most popular entity\n",
        "- Machine Learning 1 (Similarity between entity and mention)\n",
        "- Machine Learning 2 Joint Assignmnet - if Clinton and Sanders appear together than it must be Hillary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxu-sgM6D2AZ"
      },
      "source": [
        "## TagMe\n",
        "\n",
        "is a powerful tool that identifies on-the-fly meaningful substrings (called \"spots\") in an unstructured text and link each of them to a pertinent Wikipedia page in an efficient and effective way. You can annotate a text by issuing a query to the API documented in this page.\n",
        "\n",
        "**The annotation service lets you find entities mentioned in a text and link them to Wikipedia**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJk-BW-EO0Su"
      },
      "source": [
        "!pip install tagme"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "PUrj0olhD2Aa"
      },
      "source": [
        "\n",
        "import tagme\n",
        "# Set the authorization token for subsequent calls.\n",
        "#https://sobigdata.d4science.org/web/tagme/tagme-help\n",
        "tagme.GCUBE_TOKEN = \"x\"\n",
        "\n",
        "article = dataset[4].split(\"\\t\")[3]\n",
        "\n",
        "annotated_article = tagme.annotate(article)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZCytBmxD2Aa"
      },
      "source": [
        "print(type(annotated_article))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0SrCt2UD2Ab"
      },
      "source": [
        "annotated_article"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hyl-CKBHD2Ab"
      },
      "source": [
        "Annotations are associated a rho-score indicating the likelihood of an annotation being correct\n",
        "In the example, we discard annotations with a score lower than 0.1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80j7taApD2Ac"
      },
      "source": [
        "for ann in annotated_article.get_annotations(0.1): \n",
        "    print (ann)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iuFunhYD2Ac"
      },
      "source": [
        "# test with this\n",
        "sent = tagme.annotate(\"Yesterday I watched the debate between Clinton and Sanders.\")\n",
        "\n",
        "# Print annotations with a score higher than 0.1\n",
        "for ann in sent.get_annotations(0.1):\n",
        "    print (ann)\n",
        "\n",
        "# why is it still making mistakes?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rz-YR4vD2Ac"
      },
      "source": [
        "# computing entity relatedness\n",
        "rels = tagme.relatedness_title((\"Hillary Clinton\", \"Bernie Sanders\"))\n",
        "print (\"Hillary and Bernie have a semantic relation of\", rels.relatedness[0].rel)\n",
        "\n",
        "rels = tagme.relatedness_title((\"Bill Clinton\", \"Bernie Sanders\"))\n",
        "print (\"Bill and Bernie have a semantic relation of\", rels.relatedness[0].rel)\n",
        "\n",
        "rels = tagme.relatedness_title((\"Bill Clinton\", \"Hillary Clinton\"))\n",
        "print (\"Bill and Hillary have a semantic relation of\", rels.relatedness[0].rel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehRG0_OA8Arm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snDAOe5MsXtk"
      },
      "source": [
        "# Dictionary Methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-cekHHfsXtu"
      },
      "source": [
        "#we want to start with a single string object - once we write sth we like, we scale up \n",
        "s1=\"Provides for the enforcement of all provisions of law that impose sanctions against countries engaged in gross violations of human rights and that support terrorism against Iraq. Calls on the President to seek multilateral cooperation to: (1) deny dangerous technologies to Iraq; and\"\n",
        "print(s1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "59rRGzifsXtx"
      },
      "source": [
        "# we define a list of women-related words - we put in Iraq. to make sure the test works (we will drop it later)\n",
        "female_dict=[\"woman\",\"women\",\"female\",\"girl\",\"girls\",\"Iraq.\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "bsd4NYXbsXty"
      },
      "source": [
        "# empty list for all women-related things mentions\n",
        "all_women=[]\n",
        "\n",
        "# we will loop over our femal dict list -> each time, the list comprehension will loop over s1 and add to all_women\n",
        "# all instances in which that particular word from female dictionary was found.  Note that .extend takes the elements \n",
        "# of a new list and adds them to an existing list as individual elements, not as new list-element (producing nested lists)\n",
        "# as .append would - experiment with .append! \n",
        "\n",
        "for word in female_dict:\n",
        "    all_women.extend([x for x in s1.split() if x==word])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpuoJDNHsXty"
      },
      "source": [
        "print(\"I found \", len(all_women), \"cases of women mentioned: \", all_women)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "ToGqEPXqsXtz"
      },
      "source": [
        "# now that we know this sort of works, let's fine-tune, specifically, let's clean up and lower text\n",
        "# I will not throw out stop words since for this example I do not care that text contains them\n",
        "\n",
        "# remember you need regular expressions loaded for some of the commands!\n",
        "\n",
        "import re\n",
        "\n",
        "def clean_sanctions(sometextasargument):\n",
        "    # note that whatever we pass to function as argument when invoked will become assigned to object sometextasargument, \n",
        "    # will then be assigned to object text which will keep changing until we assign it to object sometextasoutput\n",
        "    text=sometextasargument.replace('\\n\\n','')\n",
        "    text=text.lower()\n",
        "    text= re.sub('[^A-Za-z0-9]+', ' ', text)\n",
        "    text=re.sub(r'[0-9]+', \" \", text)     \n",
        "    sometextasoutput=text\n",
        "    # finally, we ask the function returns what it did:  \n",
        "    return (sometextasoutput)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "0ExZg1KYsXtz"
      },
      "source": [
        "# let's check what fn returns by passing to it the string object s1\n",
        "# a remark on the function - I could have used \"text\" throughout so no \"sometextasargument\", \"sometextasoutput\"\n",
        "# to the same effect - make sure you see that this works too\n",
        "# note that unless you tell the fn what you want returned, it will perform the things you said and return...nothing! so \"return!\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ExlTj_wsXt0"
      },
      "source": [
        "print(clean_sanctions(s1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gq_iZ6K9sXt1"
      },
      "source": [
        "# note that now I should specify my dictionary list again so that it contains only lower case words with no punctuation:\n",
        "\n",
        "female_dict=[\"woman\",\"women\",\"female\",\"girl\",\"girls\",\"iraq\"]\n",
        "\n",
        "all_women=[]\n",
        "\n",
        "# note also that now I split the output of the function, clean_sanctions(s1), and not the original string s1!\n",
        "\n",
        "for word in female_dict:\n",
        "    all_women.extend([x for x in clean_sanctions(s1).split() if x==word])\n",
        "print(\"I found \", len(all_women), \"cases of women mentioned: \", all_women)\n",
        "\n",
        "#you will notice that now I find more - because I cleaned up the text beforehand"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpSIvqlQsXt2"
      },
      "source": [
        "import csv\n",
        "\n",
        "tsv_file = open(\"/content/drive/MyDrive/Colab Notebooks/Oxford Text Analysis/Data/sascat_excerpt.tsv\")\n",
        "read_tsv = csv.reader(tsv_file, delimiter=\"\\t\")\n",
        "all_lines=[]\n",
        "for line in read_tsv:\n",
        "    print(line)\n",
        "    all_lines.append(line)\n",
        "tsv_file.close() # we see here each row is now saved as a list, in a list: all_lines"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMl-7v3Iuawg"
      },
      "source": [
        "len(all_lines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbhakbcosXt3"
      },
      "source": [
        "#let's look at where text is, row 2, cell 11:\n",
        "\n",
        "all_lines[1][10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EgcKayXsXt6"
      },
      "source": [
        "all_lines[0][10]# we do not need top row - it has datanames (after pop, all_lines[0][10] will produce text above, formerly all_lines[1][10])\n",
        "all_lines.pop(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "UFYjBxBesXt7"
      },
      "source": [
        "#and note that what I want returned is a string of the number of times mentioned and the list with mentions\n",
        "\n",
        "def women_mentions(argument):\n",
        "    female_dict=[\"woman\",\"women\",\"female\",\"girl\",\"girls\",\"iraq\"]\n",
        "    all_women=[]\n",
        "    for word in female_dict:\n",
        "        all_women.extend([y for y in clean_sanctions(argument).split() if y==word])\n",
        "    return (str(len(all_women)),all_women)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYDzquIQa3NM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntCqu81csXt7"
      },
      "source": [
        "#good to check\n",
        "print(women_mentions(all_lines[0][10]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "07bSuljEsXt8"
      },
      "source": [
        "f1 = open(\"sascat_women.txt\",\"w\",encoding=\"utf8\")\n",
        "\n",
        "# remember all_lines is the big list and now x will be the nested list (row) on each iteration\n",
        "# we need to save: id-snippet identifier, and the product of the search for women in 11th column\n",
        "\n",
        "for x in all_lines:\n",
        "    f1.write(x[0]+\"\\t\"+str(women_mentions(x[10]))+\"\\n\")\n",
        "\n",
        "f1.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXybOnP2CA9H"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gee1riC18GzV"
      },
      "source": [
        "# Pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2mStYi78GzW"
      },
      "source": [
        "\n",
        "![pandas1](https://media.giphy.com/media/Wa5JDuv6kzoTC/giphy.gif)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wmsphy8T8Gze"
      },
      "source": [
        "## Why is Pandas AMAZING?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQcUQ43I8Gzf"
      },
      "source": [
        "1. super easy loading in data\n",
        "2. easy data cleaning\n",
        "3. easy data manipulation\n",
        "4. easy merging and extraction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utfiZhrx8Gzf"
      },
      "source": [
        "### Lets learn to load data in pandas\n",
        "\n",
        "We are going to start with yesterdays data - German Sputnik Newspaper articles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WYakho08Gzf"
      },
      "source": [
        "**Lets open the dataset from yesterday using Python's open function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IndhKKua8Gzg"
      },
      "source": [
        "with open('/content/drive/MyDrive/Colab Notebooks/Oxford Text Analysis/Data/sputnikgerman20.tsv','r') as tsv:\n",
        "    data = [line.strip().split('\\t') for line in tsv]\n",
        "    \n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "xLgeALL88Gzg"
      },
      "source": [
        "#get the date\n",
        "data_dates = [el[2] for el in data]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDtyibe_8Gzh"
      },
      "source": [
        "data_dates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tpF_DYL8Gzh"
      },
      "source": [
        "**Lets try with pandas**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXRrZBZ_8Gzi"
      },
      "source": [
        "sputnikdata1 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Oxford Text Analysis/Data/sputnikgerman20.tsv')\n",
        "\n",
        "#why do we get an error message?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUT8UdCA8Gzi"
      },
      "source": [
        "sputnikdata1 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Oxford Text Analysis/Data/sputnikgerman20.tsv',sep='\\t')\n",
        "sputnikdata1.head()\n",
        "#what do we want to change here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WpbkMbO8Gzj"
      },
      "source": [
        "sputnikdata = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Oxford Text Analysis/Data/sputnikgerman20.tsv',sep='\\t', header=None)\n",
        "sputnikdata"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zybuXBSz8Gzj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wev-KKmo8Gzj"
      },
      "source": [
        "## Select Column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iPpM7mr8Gzj"
      },
      "source": [
        "### how would I select the date?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb2wPGoE8Gzj"
      },
      "source": [
        "sputnikdata_date=sputnikdata[2]\n",
        "sputnikdata_date"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHjyjyye8Gzk"
      },
      "source": [
        "### Select only content and title\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmeVBQxw8Gzk"
      },
      "source": [
        "sputnikdata_content_title=sputnikdata[[3,4]]\n",
        "sputnikdata_content_title"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1jR9p9q8Gzk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hs_Lgjit8Gzk"
      },
      "source": [
        "## Select rows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5vsKwQt8Gzk"
      },
      "source": [
        "#select 0-5\n",
        "\n",
        "sputnikdata_rows=sputnikdata[0:6]\n",
        "sputnikdata_rows"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsOu4_mP8Gzl"
      },
      "source": [
        "sputnikdata_content_title=sputnikdata[[3,4]]\n",
        "sputnikdata_content_title"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6uGWVDG8Gzl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ccumusf8Gzl"
      },
      "source": [
        "sputnikdata_content_title=sputnikdata[[3,4]]\n",
        "sputnikdata_content_title"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLci_F8q8Gzl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaIN-P548Gzm"
      },
      "source": [
        "sputnikdata_date=sputnikdata[2]\n",
        "sputnikdata_date"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hz5latm78Gzm"
      },
      "source": [
        "sputnikdata_content_title=sputnikdata[[3,4]]\n",
        "sputnikdata_content_title"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRP3B3sF8Gzo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHx-wzbz8Gzo"
      },
      "source": [
        "## Clean Data\n",
        "\n",
        "1. lowercase\n",
        "2. remove numbers \n",
        "3. remove punctuation\n",
        "3. remove stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfOHG40h8Gzp"
      },
      "source": [
        "sputnikdata.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxApd8Mb8Gzp"
      },
      "source": [
        "#remove not needed columns\n",
        "sputnikdata2 = sputnikdata.drop(sputnikdata.columns[[0, 1,4, 5,6,7,8]], axis=1)  # df.columns is zero-based pd.Index \n",
        "sputnikdata2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-4UWPrL8Gzp"
      },
      "source": [
        "#lowercase\n",
        "sputnikdata2[\"lowercased\"]=sputnikdata2[3].str.lower()\n",
        "sputnikdata2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "FXM2rwCL8Gzp"
      },
      "source": [
        "## remove punctuation\n",
        "sputnikdata2[\"nopunct\"] = sputnikdata2['lowercased'].str.replace('[^\\w\\s]','')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09rrpUoo8Gzq"
      },
      "source": [
        "sputnikdata2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "M9Xcu1dr8Gzq"
      },
      "source": [
        "#remove numbers\n",
        "\n",
        "sputnikdata2['nonumb'] = sputnikdata2['nopunct'].str.replace('\\d+', '')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YK4HYQq8Gzq"
      },
      "source": [
        "sputnikdata2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "GX7mQV198Gzq"
      },
      "source": [
        "#remove stopwords\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('german')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdnVsdAJ8Gzq"
      },
      "source": [
        "sputnikdata2['without_stopwords'] = sputnikdata2['nonumb'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
        "sputnikdata2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "hoxcobGj8Gzr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LE0RPivF8Gzr"
      },
      "source": [
        "from collections import Counter\n",
        "counts= Counter(\" \".join(sputnikdata2[\"without_stopwords\"]).split()).most_common(10)\n",
        "counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwgaJvZj8Gzr"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "pd.DataFrame(counts, columns=['lbl','val']).set_index('lbl').plot(kind='bar');\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lR3CH5Y8Gzr"
      },
      "source": [
        "pd.Series(' '.join(sputnikdata2[3]).lower().split()).value_counts()[:10]\n",
        "#what happened here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QF975zQs8Gzs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Toh06g9v8Gzs"
      },
      "source": [
        "## Exporting data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "akf6AXye8Gzs"
      },
      "source": [
        "sputnikdata2.to_csv(\"mydata.csv\", sep='\\t', encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZ52AmIu8Gzu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nak3Z5SeP9Or"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIslczpYVgfT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNWYqoJWVgk9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSbrpcgDVhSV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQf4_W1qP6Zu"
      },
      "source": [
        "#Sources\n",
        "#https://medium.com/@makcedward/nlp-pipeline-sentence-tokenization-part-6-86ed55b185e6\n",
        "#Federico Nanni Computational Text Analysis Mannheim (2017,2018)\n",
        "#Denny, M., & Spirling, A. (2018). Text Preprocessing For Unsupervised Learning: Why It Matters, When It Misleads, And What To Do About It. Political Analysis, 26(2), 168-189. doi:10.1017/pan.2017.44\n",
        "#https://towardsdatascience.com/stemming-lemmatization-what-ba782b7c0bd8\n",
        "#https://github.com/makcedward/nlp/blob/master/sample/nlp-stemming.ipynb\n",
        "#https://github.com/makcedward/nlp/blob/master/sample/nlp_lemmatization.ipynb\n",
        "#https://www.datacamp.com/community/tutorials/stemming-lemmatization-python\n",
        "#https://opendatagroup.github.io/data%20science/2019/03/21/preprocessing-text.html\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}