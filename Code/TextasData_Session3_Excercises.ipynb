{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of TextasData_Session3_Excercises_Solutions.ipynb",
      "provenance": [],
      "mount_file_id": "1_s9CcQ0JiGalLC8gNqK3DoAIAXmSzclQ",
      "authorship_tag": "ABX9TyMi0RSrmqMwqp5qhsmEJvii",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aelshehawy/text-as-data-computational-text-analysis-oxford/blob/main/Code/TextasData_Session3_Excercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUnkw7XFLLZI"
      },
      "source": [
        "**Excercise 1 - Word-Embeddings**\n",
        "\n",
        "Using Pandas and sascat_excerpts\n",
        "\n",
        "\n",
        "- Use the word embeddings on your content column --> create a new column that reflects the content with the word embeddings after text-preprocessing steps\n",
        "- Make sure to remove all unnecessary columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaarNNH0NWfH"
      },
      "source": [
        "**Excercise 2 - Word-Embeddings and Cosine-Similarity**\n",
        "\n",
        "Use the rt_dataset_small.tsv <br>\n",
        "\n",
        "a) Open the file <br>\n",
        "b) extract the content of the first article and save it as a string <br>\n",
        "c) remove this article from the list of articles of the rt_dataset_small<br>\n",
        "b) restrict the list of articles of the rt_dataset_small to 50 articles<br>\n",
        "c) use single function that takes a law/document (one row), cleans the article and creates the article-embeddings. \n",
        "\n",
        "- You need to embed the content of the first article you extracted. Save one embedding for this article.\n",
        "- You also need to embed the content of each of the 50 articles from the rt corpus, save the embedding of each article as the last item of the list (Remember we have a list of lists, one list for each article)\n",
        "\n",
        "\n",
        "c) calculate cosine-similarity between the article embedding and each of the 50 articles' embedding. The cosine similarity can be saved as another item in the list of lists of articles that you have.\n",
        "\n",
        "PS - You might need to create a function that reshapes the numpy array of the embedding before this step. Read error messages carefully! Stackoverflow is your best friend.\n",
        "\n",
        "What is the most\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJCemzZzLmCw"
      },
      "source": [
        "**Excercise 3 - Clustering Visualization**\n",
        "\n",
        "- Visualize the clusters in rt_dataset_small"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrT_Qs50LmCw"
      },
      "source": [
        "**Excercise 4 - Topic Modelling**\n",
        "\n",
        "- Use sascat_expert\n",
        "- Think about what could be the appropriate text preprocessing steps\n",
        "- Discover what are the topics (10) using LDA of the sascat dataset\n",
        ")\n",
        "- Please comment on what you would name each topic?"
      ]
    }
  ]
}